{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e797a8-8c38-4d93-b365-9c24359d0abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm==0.9.2 torch torchvision pandas scikit-learn matplotlib xgboost easyocr --quiet seaborn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f1b1ef-9747-4129-8961-d53c6f500ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manshusainishab/Library/Python/3.11/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import xgboost as xgb\n",
    "\n",
    "import easyocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72def7-35bd-47c4-bc39-28120f3d025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set your local paths here (CHANGE these to your real paths)\n",
    "BASE_PATH = \"/Users/manshusainishab/Desktop/dataset/train\"     # example path\n",
    "CSV_PATH = \"/Users/manshusainishab/Desktop/dataset/images_numeric_labels.csv\"\n",
    "\n",
    "IMAGE_DIR = BASE_PATH\n",
    "\n",
    "# Load dataframe\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Keep only filename (remove full path)\n",
    "df['image_path'] = df['image_path'].apply(os.path.basename)\n",
    "\n",
    "print(\"Total samples:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9567f8e-f978-42e4-9520-a5f382668d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "class AadhaarDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.img_dir, row['image_path'])\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, row['image_path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ccbe0-5887-4bf7-ae78-ec43e89cbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AadhaarDataset(df, IMAGE_DIR, transform) \n",
    "train_size = int(0.8 * len(dataset)) \n",
    "test_size = len(dataset) - train_size \n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size]) \n",
    "train_indices = train_dataset.indices \n",
    "test_indices = test_dataset.indices \n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=8) \n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cf558-8d3d-45f5-b9a0-73e6d871c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "num_ftrs = model.head.in_features\n",
    "model.head = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cda0f-c271-4abf-b988-a67a89cd55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optim, device):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0\n",
    "\n",
    "    for imgs, labels, _ in loader:\n",
    "        imgs, labels = imgs.to(device), labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(labels)\n",
    "        loss_sum += loss.item() * len(labels)\n",
    "\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0\n",
    "    preds_all, labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, _ in loader:\n",
    "            imgs, labels = imgs.to(device), labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_sum += loss.item()*len(labels)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "            preds_all.extend(preds.cpu().numpy().flatten())\n",
    "            labels_all.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    return loss_sum/total, correct/total, preds_all, labels_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb2ed7-c8f4-49be-b3d1-f466240ab9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    tl, ta = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    vl, va, _, _ = eval_model(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1} | Train Acc: {ta:.4f} | Test Acc: {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5751f6-04e8-4da5-a22f-777337bc0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, preds, labels = eval_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b03685-ecc7-4dd7-9615-393db746c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stage 2 check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dad8d-4dcd-4a60-8956-09d7aaefa544",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers --quiet\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df3f1b-387b-4f59-b918-d2c9c69205f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "TESS_LANGS = \"eng+hin+tam+tel+kan+mal+ben+mar+pan+guj\"\n",
    "\n",
    "def extract_text(path):\n",
    "    text = pytesseract.image_to_string(\n",
    "        Image.open(path),\n",
    "        lang=TESS_LANGS\n",
    "    )\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca76701-8807-4ef4-a6fa-c6acad867399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "INDIAN_SCRIPTS = {\n",
    "    \"devanagari\",  # hi, mr, ne, sa, bho, mai\n",
    "    \"bengali\",     # bn, as, mni\n",
    "    \"tamil\",       # ta\n",
    "    \"telugu\",      # te\n",
    "    \"kannada\",     # kn\n",
    "    \"malayalam\",   # ml (your model doesn't support OCR for it)\n",
    "    \"oriya\",       # odia (not supported in EasyOCR)\n",
    "    \"gurmukhi\",    # Punjabi (not supported)\n",
    "}\n",
    "\n",
    "LATIN = \"latin\"\n",
    "\n",
    "def detect_scripts(text):\n",
    "    scripts = set()\n",
    "\n",
    "    for ch in text:\n",
    "        if ch.isspace() or ch.isdigit():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            block = unicodedata.name(ch)\n",
    "        except:\n",
    "            scripts.add(\"other\")\n",
    "            continue\n",
    "\n",
    "        # Indian scripts\n",
    "        if \"DEVANAGARI\" in block:\n",
    "            scripts.add(\"devanagari\")\n",
    "        elif \"BENGALI\" in block:\n",
    "            scripts.add(\"bengali\")\n",
    "        elif \"TAMIL\" in block:\n",
    "            scripts.add(\"tamil\")\n",
    "        elif \"TELUGU\" in block:\n",
    "            scripts.add(\"telugu\")\n",
    "        elif \"KANNADA\" in block:\n",
    "            scripts.add(\"kannada\")\n",
    "        elif \"MALAYALAM\" in block:\n",
    "            scripts.add(\"malayalam\")\n",
    "        elif \"ORIYA\" in block or \"ODIA\" in block:\n",
    "            scripts.add(\"oriya\")\n",
    "        elif \"GURMUKHI\" in block:\n",
    "            scripts.add(\"gurmukhi\")\n",
    "\n",
    "        # English\n",
    "        elif \"LATIN\" in block:\n",
    "            scripts.add(\"latin\")\n",
    "\n",
    "        # Arabic family (Urdu)\n",
    "        elif \"ARABIC\" in block:\n",
    "            scripts.add(\"arabic\")\n",
    "\n",
    "        else:\n",
    "            scripts.add(\"other\")\n",
    "\n",
    "    return scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7b1b4-a391-43f7-87a2-3586961875a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowed_mix(scripts):\n",
    "    indian = scripts.intersection(INDIAN_SCRIPTS)\n",
    "    english = \"latin\" in scripts\n",
    "\n",
    "    # FOREIGN language detected ‚Üí reject\n",
    "    if scripts - indian - {\"latin\",\"other\"} :\n",
    "        return False\n",
    "\n",
    "    # English + exactly one Indian language hindi -> ok\n",
    "    if (english and len(indian) == 1 and indian == {\"devanagari\"}):\n",
    "        return True\n",
    "\n",
    "    # Anything else ‚Üí reject\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb641a4-f0ee-474e-b892-eb61541669c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def text_embedding(text):\n",
    "    vector = embedder.encode(text)\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9fd76-3b0f-4634-a691-a5160db1d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectors = []\n",
    "y_labels = []\n",
    "\n",
    "print(\"Extracting OCR + embeddings for all images...\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore specific user warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"'pin_memory' argument is set as true but not supported on MPS now\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    path = os.path.join(IMAGE_DIR, row[\"image_path\"])\n",
    "    text = extract_text(path)\n",
    "    emb = text_embedding(text)     # 384-d vector\n",
    "    X_vectors.append(emb)\n",
    "    y_labels.append(row[\"label\"])\n",
    "\n",
    "X_vectors = np.vstack(X_vectors)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "print(\"Embedding matrix shape:\", X_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350cd93-c379-4187-8c9e-813640e3b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.zeros(len(df), dtype=bool)\n",
    "test_mask = np.zeros(len(df), dtype=bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "X_train = X_vectors[train_mask]\n",
    "X_test = X_vectors[test_mask]\n",
    "y_train = y_labels[train_mask]\n",
    "y_test = y_labels[test_mask]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f750c-8460-4f42-bf79-39ffab502f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"binary:logistic\"\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "def predict_pipeline(path):\n",
    "    # ---------- Stage 1: ViT ----------\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = torch.sigmoid(model(img_t)).item()\n",
    "        vit_pred = 1 if out >= 0.65 else 0\n",
    "\n",
    "    # If ViT rejects ‚Üí stop here\n",
    "    if vit_pred == 0:\n",
    "        return {\n",
    "            \"final\": 0,\n",
    "            \"reason\": \"ViT rejected (Stage 1)\",\n",
    "            \"stage1_vit\": vit_pred,\n",
    "            \"stage2_xgb\": None,\n",
    "            \"ocr_text\": None,\n",
    "        }\n",
    "\n",
    "    # ---------- Stage 2: OCR + Embedding + XGBoost ----------\n",
    "    # ---------------- Stage 2: OCR + Language Filter + Embedding + XGB ----------------\n",
    "\n",
    "    text = extract_text(path)\n",
    "    scripts = detect_scripts(text)\n",
    "    \n",
    "    if not allowed_mix(scripts):\n",
    "        return {\n",
    "            \"final\": 0,\n",
    "            \"reason\": f\"Rejected: unsupported script combination {scripts}\",\n",
    "            \"stage1_vit\": vit_pred,\n",
    "            \"stage2_xgb\": None,\n",
    "            \"ocr_text\": text\n",
    "        }\n",
    "\n",
    "    # 2Ô∏è‚É£ Continue with your embedding + XGB\n",
    "    emb = text_embedding(text).reshape(1, -1)\n",
    "    \n",
    "    xgb_pred = int(xgb_model.predict(emb)[0])\n",
    "    prob = float(xgb_model.predict_proba(emb)[0, 1])\n",
    "    \n",
    "    return {\n",
    "        \"stage1_vit\": vit_pred,\n",
    "        \"stage2_xgb\": xgb_pred,\n",
    "        \"xgb_prob\": prob,\n",
    "        \"ocr_text\": text,\n",
    "        \"script_detected\": list(scripts),\n",
    "        \"final\": xgb_pred}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9734f-dc27-4708-bbe1-201fc1749085",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"/Users/manshusainishab/Desktop/dataset/train/adhar_c66.png\"\n",
    "result = predict_pipeline(sample)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833343d0-9ff8-46c2-8019-001d6e94a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"/Users/manshusainishab/Downloads/image_128.png\"\n",
    "result = predict_pipeline(sample)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd996d22-d98d-4692-9ebb-994ce2ece772",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "final_labels = []\n",
    "\n",
    "print(\"Evaluating full pipeline (Stage1 + Stage2)...\")\n",
    "\n",
    "for idx in test_indices:\n",
    "    row = df.iloc[idx]\n",
    "    path = os.path.join(IMAGE_DIR, row[\"image_path\"])\n",
    "    true_label = row[\"label\"]\n",
    "\n",
    "    result = predict_pipeline(path)\n",
    "    final_pred = result[\"final\"]\n",
    "\n",
    "    final_labels.append(true_label)\n",
    "    final_preds.append(final_pred)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "final_labels = np.array(final_labels)\n",
    "final_preds = np.array(final_preds)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nüìò Classification Report (Final Output)\")\n",
    "print(classification_report(final_labels, final_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(final_labels, final_preds)\n",
    "print(\"\\nüìä Confusion Matrix (Final):\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Pred 0 (Reject)', 'Pred 1 (Aadhaar)'],\n",
    "            yticklabels=['True 0 (Non-Aadhaar)', 'True 1 (Aadhaar)'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Final Confusion Matrix (After Stage1 + Stage2)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d1bfb-41c6-4001-a2a9-b058da9aa0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73255efa-e89e-4ce1-a6e1-f017daad789b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
